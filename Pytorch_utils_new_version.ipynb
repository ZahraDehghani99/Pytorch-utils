{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgtbCaITu2cJcJCLFBhXd3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraDehghani99/Pytorch-utils/blob/main/Pytorch_utils_new_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train, valid and test function in Pytorch\n"
      ],
      "metadata": {
        "id": "a8CW-0h7jLdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train(model, train_dataloader):\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for batch in train_dataloader:\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      outputs = model(**batch)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      progress_bar.update(1)\n",
        "      total_loss += loss.item()\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "def evaluate(model, valid_dataloader):\n",
        "  model.eval()\n",
        "  total_val_loss = 0\n",
        "  total_correct = 0\n",
        "  total_samples = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in valid_dataloader:\n",
        "      for i in batch:\n",
        "        batch[i] = batch[i].to(device)\n",
        "      outputs = model(**batch)\n",
        "      loss = outputs.loss\n",
        "\n",
        "      total_val_loss += loss.item()\n",
        "\n",
        "      # Calculate accuracy\n",
        "      _, predicted_labels = torch.max(outputs.logits, dim=1)\n",
        "      predicted_labels = predicted_labels.to(device)\n",
        "      correct = (predicted_labels == batch[\"labels\"]).sum().item()\n",
        "      total_correct += correct\n",
        "      total_samples += batch[\"labels\"].size(0)\n",
        "\n",
        "  return total_val_loss, total_correct, total_samples\n",
        "\n",
        "def test_model(model, test_dataloader):\n",
        "  y_pred_test = []\n",
        "  y_actual_test = []\n",
        "  total_correct, total_samples = 0, 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    n_true = 0\n",
        "    n_total = 0\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "      for i in batch:\n",
        "        batch[i] = batch[i].to(device)\n",
        "      outputs = model(**batch)\n",
        "      _, y_predtest = torch.max(outputs.logits,1)\n",
        "\n",
        "      y_predtest = y_predtest.to(device)\n",
        "      correct = (y_predtest == batch[\"labels\"]).sum().item()\n",
        "      total_correct += correct\n",
        "      total_samples += batch[\"labels\"].size(0)\n",
        "\n",
        "      for i in range(len(y_predtest)):\n",
        "        y_pred_test.append(y_predtest[i])\n",
        "        y_actual_test.append(batch[\"labels\"][i])\n",
        "\n",
        "    print(f'Test accuracy : {(total_correct/total_samples) *100 :.2f}%')\n",
        "    return y_pred_test, y_actual_test"
      ],
      "metadata": {
        "id": "k0g5Zpcii8cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "FI2qw4_XkXJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "\n",
        "  total_loss_train = train(model, train_dataloader)\n",
        "  avg_loss_train = total_loss_train/ len(train_dataloader)\n",
        "  total_val_loss, total_correct, total_samples = evaluate(model, valid_dataloader)\n",
        "  avg_loss_valid = total_val_loss /len(valid_dataloader)\n",
        "  accuracy = total_correct / total_samples\n",
        "\n",
        "  print(f'Epoch {epoch+1} : Train loss : {avg_loss_train} | Valid loss : {avg_loss_valid} | Valid accuracy : {accuracy}')\n",
        "  # Log the loss and accuracy values at the end of each epoch\n",
        "  wandb.log({\n",
        "      \"Epoch\": epoch,\n",
        "      \"Train Loss\": avg_loss_train,\n",
        "      \"Valid Loss\": avg_loss_valid,\n",
        "      \"Valid Acc\": accuracy})"
      ],
      "metadata": {
        "id": "7QVf4XnBkPMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train, valid and test functions in Pytorch with Tensorboard"
      ],
      "metadata": {
        "id": "et8qDJ4sjVJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a funciton to calculate accuracy for multi class classification model\n",
        "def binary_accuracy(y_pred, y_act):\n",
        "\n",
        "  y_pred_softmax = torch.softmax(y_pred, 1)\n",
        "  _, y_pred_tag = torch.max(y_pred_softmax,1)\n",
        "\n",
        "  correct = (y_pred_tag == y_act).float()  # Show which elements are the same with 1\n",
        "  acc = correct.sum()/len(correct)\n",
        "  acc = torch.round(acc * 100)\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "--7Q_RFiuLOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyAMGQiyDewG"
      },
      "outputs": [],
      "source": [
        "# define a function for training a model\n",
        "def train(model1, train_loader):\n",
        "\n",
        "  train_epoch_loss = 0\n",
        "  train_epoch_acc = 0\n",
        "\n",
        "  # train the model\n",
        "  model1.train()\n",
        "\n",
        "  loop = tqdm(enumerate(train_loader), total = n_total_step, leave = False)\n",
        "\n",
        "  for i, batch in loop:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # retrieve text and number of words\n",
        "    text, text_lengths = batch.preprocessing_text\n",
        "\n",
        "    # convert to 1D tensor\n",
        "    y_pred = model1(text).squeeze()\n",
        "\n",
        "    loss_train = criterion(y_pred, batch.label)\n",
        "\n",
        "    acc_train = binary_accuracy(y_pred, batch.label)\n",
        "\n",
        "    loss_train.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_epoch_loss += loss_train.item()\n",
        "    train_epoch_acc += acc_train.item()\n",
        "\n",
        "    loop.set_description(f'Epoch : [{epoch}/{num_epoch}]')\n",
        "    loop.set_postfix(loss = loss_train.item(), accuracy = acc_train.item())\n",
        "\n",
        "  train_loss.append(train_epoch_loss/len(train_loader))\n",
        "  train_acc.append(train_epoch_acc/len(train_loader))\n",
        "\n",
        "  # if you want to monitor your model in tensorboard\n",
        "  writer.add_scalar('training loss',train_epoch_loss/len(train_loader) , epoch)\n",
        "  writer.add_scalar('training accuracy',train_epoch_acc/len(train_loader) , epoch)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "# define a function for evaluating model\n",
        "def evaluate(model1, valid_loader):\n",
        "\n",
        "  # validation\n",
        "  with torch.no_grad():\n",
        "    model1.eval()\n",
        "\n",
        "    val_epoch_loss = 0\n",
        "    val_epoch_acc = 0\n",
        "\n",
        "    for batch in valid_loader:\n",
        "\n",
        "      text, text_length = batch.preprocessing_text\n",
        "\n",
        "      y_pred_val = model1(text).squeeze()\n",
        "      loss_valid = criterion(y_pred_val, batch.label)\n",
        "      acc_valid = binary_accuracy(y_pred_val, batch.label)\n",
        "\n",
        "      val_epoch_loss += loss_valid.item()\n",
        "      val_epoch_acc += acc_valid.item()\n",
        "\n",
        "    valid_loss.append(val_epoch_loss/len(valid_loader))\n",
        "    valid_acc.append(val_epoch_acc/len(valid_loader))\n",
        "\n",
        "  return valid_loss, valid_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('model_Ham')"
      ],
      "metadata": {
        "id": "VbnDUNAhd1bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir 'model_Ham'"
      ],
      "metadata": {
        "id": "Hqioq8_sd17B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpqffeV7ctH_"
      },
      "outputs": [],
      "source": [
        "# define a function for training a model\n",
        "def train_fun(model1, train_loader):\n",
        "\n",
        "  train_epoch_loss = 0\n",
        "  train_epoch_acc = 0\n",
        "\n",
        "  # train the model\n",
        "  model1.train()\n",
        "\n",
        "  loop = tqdm(enumerate(train_loader), total = n_total_step, leave = False)\n",
        "\n",
        "  for i, batch in loop:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # retrieve text and number of words\n",
        "    text, text_lengths = batch.preprocessing_text\n",
        "\n",
        "    text = text.to(device)\n",
        "    text_lengths = text_lengths.to(device)\n",
        "\n",
        "    # convert to 1D tensor\n",
        "    y_pred = model1(text, text_lengths).squeeze()\n",
        "\n",
        "    # because our labels in form of float tensor, we should convert them to long tensor(int)\n",
        "    label = batch.label.to(torch.long)\n",
        "\n",
        "    loss_train = criterion(y_pred, label)\n",
        "\n",
        "    acc_train = multi_class_acc(y_pred, label)\n",
        "\n",
        "    loss_train.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_epoch_loss += loss_train.item()\n",
        "    train_epoch_acc += acc_train.item()\n",
        "\n",
        "    loop.set_description(f'Epoch : [{epoch}/{num_epoch}]')\n",
        "    loop.set_postfix(loss = loss_train.item(), accuracy = acc_train.item())\n",
        "\n",
        "  train_loss.append(train_epoch_loss/len(train_loader))\n",
        "  train_acc.append(train_epoch_acc/len(train_loader))\n",
        "\n",
        "  # if you want to monitor your model in tensorboard\n",
        "  writer.add_scalar('training loss',train_epoch_loss/len(train_loader) , epoch)\n",
        "  writer.add_scalar('training accuracy',train_epoch_acc/len(train_loader) , epoch)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "# define a function for evaluating model\n",
        "def evaluate(model1, valid_loader):\n",
        "\n",
        "  # validation\n",
        "  with torch.no_grad():\n",
        "    model1.eval()\n",
        "\n",
        "    val_epoch_loss = 0\n",
        "    val_epoch_acc = 0\n",
        "\n",
        "    for batch in valid_loader:\n",
        "\n",
        "      text, text_length = batch.preprocessing_text\n",
        "\n",
        "      text = text.to(device)\n",
        "      text_length = text_length.to(device)\n",
        "\n",
        "      y_pred_val = model1(text, text_length).squeeze()\n",
        "      label = batch.label.to(torch.long)\n",
        "      loss_valid = criterion(y_pred_val, label)\n",
        "      acc_valid =  multi_class_acc(y_pred_val, label)\n",
        "\n",
        "      val_epoch_loss += loss_valid.item()\n",
        "      val_epoch_acc += acc_valid.item()\n",
        "\n",
        "    valid_loss.append(val_epoch_loss/len(valid_loader))\n",
        "    valid_acc.append(val_epoch_acc/len(valid_loader))\n",
        "\n",
        "  return valid_loss, valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "3C-iexJBd79p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "PKxN2PH9j5Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_loss, valid_loss = [], []\n",
        "train_acc, valid_acc = [], []\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "\n",
        "  train_loss, train_acc = train(model1, train_loader)\n",
        "  writer.flush()\n",
        "  valid_loss, valid_acc = evaluate(model1, valid_loader)\n",
        "\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "MN6SHR_3d901"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model1, '/content/drive/MyDrive/AI-Internship/model1LSTM_hamshahri') # model1 with 10 epochs"
      ],
      "metadata": {
        "id": "fWiGIoDBd_xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and validation loss\n",
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(valid_loss, label='Validation loss')\n",
        "plt.legend(frameon=False)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss value')\n",
        "plt.title(\"Loss function for each epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sVwbCP85eBaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and validation accuracy\n",
        "plt.plot(train_acc, label='Training accuracy')\n",
        "plt.plot(valid_acc, label='Validation accuracy')\n",
        "plt.legend(frameon=False)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy value')\n",
        "plt.title(\"Accuracy for each epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BKb_rMVeeCAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training loss on last epoch : {train_loss[-1]}')\n",
        "print(f'validation loss on last epoch : {valid_loss[-1]}')\n",
        "print('-'*20)\n",
        "print(f'training accuracy on last epoch : {train_acc[-1]}')\n",
        "print(f'validation accuracy on last epoch : {valid_acc[-1]}')"
      ],
      "metadata": {
        "id": "87TA8VOReGz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model1, test_loader):\n",
        "  y_pred_test = []\n",
        "  y_actual_test = []\n",
        "\n",
        "  model1.eval()\n",
        "  with torch.no_grad():\n",
        "    n_true = 0\n",
        "    n_total = 0\n",
        "    n_class_correct = [0 for i in range(num_class)]\n",
        "    n_class_sample = [0 for i in range(num_class)]\n",
        "\n",
        "    for batch in test_loader:\n",
        "\n",
        "      text, text_length = batch.preprocessing_text\n",
        "\n",
        "      output = model1(text, text_length).squeeze()\n",
        "\n",
        "      _, y_predtest = torch.max(output,1)\n",
        "\n",
        "      n_true += (y_predtest == batch.label).sum()\n",
        "      n_total += batch.label.size(0)\n",
        "\n",
        "      for i in range(len(y_predtest)):\n",
        "        y_pred_test.append(y_predtest[i])\n",
        "        y_actual_test.append(batch.label[i].item())\n",
        "\n",
        "      for i in range(len(y_predtest)):\n",
        "        lb = int(batch.label[i].item())\n",
        "        pred = y_predtest[i]\n",
        "        if pred == lb:\n",
        "          n_class_correct[lb] += 1\n",
        "        n_class_sample[lb] += 1\n",
        "\n",
        "    print(f'accuracy of network on the {len(test_loader)} texts is : {(n_true/n_total) *100}')\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "      acc = 100.0 * n_class_correct[i] / n_class_sample[i]\n",
        "      print(f'Accuracy of {classes[i]}: {acc} %')\n",
        "\n",
        "  return y_pred_test, y_actual_test\n",
        "\n"
      ],
      "metadata": {
        "id": "n5TION9OeLM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test, y_actual_test = test(model1, test_loader)"
      ],
      "metadata": {
        "id": "2O2V-4PqeP_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}